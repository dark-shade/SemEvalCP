{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\r\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\r\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\n",
      "Parsing file: C:\\Users\\sanku\\AppData\\Local\\Temp\\tmpuxho3qef\r\n",
      "Parsing [sent. 1 len. 1632]: big deal noticed issue dvd medium usb output usb output 92 102 1 2843_0 enjoying quality provides great quality 25 32 1 3062_1 screen nice image come clear keyboard fit feel right keyboard 56 64 1 1531_3 program great like iphoto love editing capability imail incorporate address book ipod ipad imovie etc imovie 143 149 1 1669_0 like 9 punds look past great 9 punds 10 17 1 2768_0 thing learn research software would need like privacy protection warranty protection case get broken crash etc software 53 61 0 2486_1 real stand computer feel keyboard speed speed 73 78 1 526_0 caught virus completely wiped hard drive matter hour hard drive 47 57 1 1923_0 eventually battery charge unless plugged even power battery 14 21 1 411_1 asus facial recognition work window logon either window logon 41 54 1 1075_2 could perfect laptop would faster system memory radeon 5850 would ddr5 instead ddr3 ddr5 98 102 1 2708_0 start menu easiest thing navigate due stacking start menu 4 14 1 953_2 _ tried use since disc drive replaced taking back geek squad found accidently used right drive replaced first one back went get correct drive drive 242 247 0 2347_0 pc user whole life taking bit time adapt o mac finding way around o 73 75 0 2583_0 screen brightness automatically adjusts screen brightness 4 21 0 19_1 everything design o simple point o 34 36 1 49_0 due course remove hard disc new macbook pro dump belongs trash hard disc 31 40 1 333_1 graphic bad lower end macbook pro spectrum easily capable running starcraft ii game comparable graphic graphic 151 159 0 2731_1 first one sent touchpad work second sent usb work third sent touchpad work fourth sent arrived yet usb 58 61 1 2349_0 led backlit display make picture pop much led backlit display 4 23 1 1258_1 seemed nice laptop except able load garmin gps software microsoft office 2003 microsoft office 2003 91 112 1 451_1 easy pick use take long get used mac o use 37 40 1 1723_0 performance awesome performance 4 15 1 446_0 love glass touchpad glass touchpad 11 25 1 2337_0 touch pad fine real touch pad touch pad 4 13 1 1764_0 feature necessary college able added onto computer feature 15 23 1 2096_1 price premium little much start looking feature worth added cash feature 70 78 1 1184_0 printing either word processor adventure word processor 27 41 1 1958_0 especially like keyboard chiclet type key keyboard 22 30 1 1418_0 _ toshiba aware issue unless extended warrenty bought toshiba nothing extended warrenty 47 64 1 1895_2 overall feel netbook poor quality poor performance although great battery life work battery life 95 107 1 1206_0 honest compatibility issue little quirk make think buy pc next time compatibility 22 35 1 2416_0 thought white mac computer looked dirty quicly use mousepad place hand typing mousepad 76 84 0 2787_0 overall computer easy use screen perfect great computer daughter love screen 46 52 1 2083_1 priced reasonable work well right box work 32 37 1 515_0 said computer covered extended warranty product with _ extended warranty taken care third party sony anymore extended warranty 45 62 0 2417_0 even though computer larger make keyboard larger keyboard 57 65 1 2157_1 computer exceptionally thin screen size processing power processing power 61 77 1 1765_1 lot memory great battery life battery life 35 47 1 2456_0 good playing apps facebook watching movie watching movie 44 59 1 1438_3 externally key keyboard falling us paint rubbing button mouse pad heals hand sit screen terrible glare screen 170 176 1 2462_0 fast one program open sixteen open program 27 34 0 2898_0 toshiba satellite expected price price 60 65 1 1446_0 called toshiba would anything even tried charge 35 phone call even though offer technical support technical support 136 153 1 1289_0 paid visit lg notebook service center alexandra road hoping make hinge tighter lg notebook service center 22 48 0 2350_1 learning finger option mousepad allow quicker browsing web page browsing 72 80 1 1491_0 backlit key wonderful working dark backlit key 4 16 1 763_0 summary laptop 2 month blue power adapter stop working power adapter 64 77 1 354_0 system constantly overheats battery life short case coming apart core application use every day work graphic artist run poorly battery life 37 49 1 2913_0 screen size perfect portable use environment screen size 0 11 1 1336_1 laptop mobile phone look chic cool sale support terrible look 37 41 1 1614_1 believe quality mac worth price price 49 54 1 2173_0 program came ilife iwork set beginning program 13 21 0 1349_0 say could motherboard dell motherboard 25 36 1 2917_0 notebook lacking hdmi port video port would enable one hook tv hdmi port 26 35 1 2274_0 use built tool often itunes open nearly every day work great iphone built tool 11 25 0 2448_0 picture clear picture 4 12 1 2352_0 key contributor led mac art aspect art aspect 44 54 1 2090_0 great product easy use great graphic graphic 42 50 1 452_1 window linux user love size screen big enough use internet artwork yet small enough reasonably portable screen 74 80 1 1476_1 image great soud excelent soud 28 32 1 2853_0 took toshiba tech support 4 call 4 different tech correct problem toshiba tech support 8 28 1 2057_0 disappointed find model discontinued apparently known motherboard problem motherboard 95 106 1 2351_0 granted still new laptop comparison previous laptop desktop mac boot noticeably quicker boot 100 108 1 2571_0 get ton compliment size speaking someone regularly commute bus attest fact perfect size computer restricted width body computing room size 34 38 1 2585_1 easy type feel great besides added feature keyboard lighted feature 63 70 1 2455_0 acer one computer bought 17 ince screen hard find lap top bag like big screen 17 ince screen 39 53 0 520_0 battery went bad year half cost around eighty hundred dollar battery 3 10 1 982_2 _ fixed dc jack inside unit rewired dc jack outside laptop replaced power brick power brick 112 123 0 2315_1 o x solid lot innovation quicklook save heap time quicklook 47 56 1 1502_2 honest use computer anything like graphic editing complex data analysis gaming complex data analysis 81 102 0 626_0 mainboard broken wait new one mainboard 4 13 1 113_2 fact somehow never opened speck dust something got inside screen permanently behind front screen way display display 180 187 1 1977_0 saw walmart computer 650 still knowing know would buy price price 112 117 1 2100_3 run window via bootcamp couple program want buy mac version like cad program bootcamp 18 26 0 533_0 still stupid bluetooth mouse bluetooth mouse 25 40 1 984_0 keep hard drive replaced hard drive 28 38 1 9_0 apple team also assist nicely choosing computer right emojihappy apple team 4 14 1 2581_0 nice learn feature happy sophisticated computer feature 37 45 0 2065_0 purchasing thing find need special interface device connect camera can not purchased store line interface device 62 78 1 280_0 many kind software necessary working person available can not downloaded software 14 22 1 2203_0 thank best buy putting computer together installing first software guy great software 76 84 0 1427_2 best buy 5 star sale service respect old men familiar technology dell computer 3 star dell support owes couple dell support 128 140 1 922_0 bought notebook 3 month overload update boot mgr boot mgr 87 95 0 1747_0 downfall sharp edge edge 17 22 1 809_0 buy gateway computer junk warranty company horrible warranty company 51 67 1 2969_0 laptop easy set set 28 34 1 2243_0 wonderful computer gaming gaming 28 34 1 2645_2 stupid pop window even pop ups blocked wait 5 minute webpage download best run web programming software need use without slowing pop ups 53 60 0 2537_0 needed one able carry work everyday one seems fit criterion carry 27 32 0 1020_0 would tell technician knew_exactly_what wrong listen go bunch junk get tell needed send computer technician 17 27 1 2927_1 another thing month left mouse key broke costed 175 send fix costed 73 79 0 1391_1 mouse pad button worst ever seen button 18 25 1 1414_0 graphic awful warranty even worth cheap payment computer graphic 4 12 1 379_1 loaded window 7 via bootcamp work flawlessly bootcamp 23 31 1 1746_0 screen keyboard mouse cant see spending extra money jump mac beautiful screen responsive island backlit keyboard fun multi touch mouse worth extra money alone screen 0 6 0 3036_0 pricing competitive pricing 4 11 1 2668_0 thing say touch pad doesnt work like time touch pad 37 46 1 1070_0 also issued replacement rma dead pixel upper zone screen noticable screen 80 86 1 1906_0 _ day received computer back screen froze screen 58 64 1 1742_1 size know 13 small especially desktop replacement external monitor care external monitor 76 92 0 435_0 sent back huge crack still work work 62 66 1 87_1 easy operate already ordered software gadget new roll royce laptop operate 14 21 1 2649_0 processor scream unique way apple osx 16 function graphic routed hardware rather software processor 4 13 1 597_1 ilife software come computer simple use produce great finished product use 64 67 1 1648_0 screen thing absolutely amazing high quality video movie gaming screen 8 14 1 3076_1 battery lasting 6 hour surfing web sunday checking football score watching funny youtube video surfing web 41 56 0 2751_0 good monitor performed well monitor 5 12 1 1450_2 best thing build computer u company like dell allow choose component better price get computer compare one apple 2000 system google dell coupon find code take signifant _ amount price\r\n",
      "\r\n",
      "*******************************************************\r\n",
      "***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***\r\n",
      "***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***\r\n",
      "***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***\r\n",
      "***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***\r\n",
      "***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***\r\n",
      "***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***\r\n",
      "*******************************************************\r\n",
      "\r\n",
      "Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...\r\n",
      "Exception in thread \"main\" edu.stanford.nlp.parser.common.NoSuchParseException\r\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:403)\r\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:375)\r\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.processResults(ParseFiles.java:272)\r\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:216)\r\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:75)\r\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1518)\r\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed : ['C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_144\\\\bin\\\\java.exe', '-mx1000m', '-cp', 'D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser-3.9.1-models.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\ejml-0.23.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\slf4j-api-1.7.12-sources.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\slf4j-api.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser-3.9.1-javadoc.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser-3.9.1-models.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser-3.9.1-sources.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser.jar', 'edu.stanford.nlp.parser.lexparser.LexicalizedParser', '-model', 'edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz', '-sentences', 'newline', '-outputFormat', 'conll2007', '-encoding', 'utf8', 'C:\\\\Users\\\\sanku\\\\AppData\\\\Local\\\\Temp\\\\tmpuxho3qef']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ce0d29fc4ec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnewText\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1900\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mdep_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mStanfordDependencyParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdep_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mdep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mdepParsingList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36mraw_parse\u001b[1;34m(self, sentence, verbose)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \"\"\"\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_parse_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mraw_parse_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36mraw_parse_sents\u001b[1;34m(self, sentences, verbose)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[1;34m'-outputFormat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_OUTPUT_FORMAT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         ]\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_trees_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtagged_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, cmd, input_, verbose)\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 stdout, stderr = java(cmd, classpath=self._classpath,\n\u001b[1;32m--> 218\u001b[1;33m                                       stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'\\xc2\\xa0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mb' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mjava\u001b[1;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Java command failed : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Java command failed : ['C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_144\\\\bin\\\\java.exe', '-mx1000m', '-cp', 'D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser-3.9.1-models.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\ejml-0.23.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\slf4j-api-1.7.12-sources.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\slf4j-api.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser-3.9.1-javadoc.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser-3.9.1-models.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser-3.9.1-sources.jar;D:\\\\Important\\\\NLP\\\\stanford-parser-full-2018-02-27\\\\stanford-parser.jar', 'edu.stanford.nlp.parser.lexparser.LexicalizedParser', '-model', 'edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz', '-sentences', 'newline', '-outputFormat', 'conll2007', '-encoding', 'utf8', 'C:\\\\Users\\\\sanku\\\\AppData\\\\Local\\\\Temp\\\\tmpuxho3qef']"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import svm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "with open('dara 1_train.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    \"\"\"\n",
    "    for row in reader:\n",
    "        print(row['example_id'])\n",
    "        print(row[' text'])\n",
    "        print(row[' aspect_term'])\n",
    "        print(row[' term_location'])\n",
    "        print(row[' class'])\n",
    "        print()        \n",
    "    \"\"\"\n",
    "    happy_emoji = [':‑)',':)',':-]',':]',':-3',':3',':->',':>','8-)','8)',':-}', ':}',':o)',':c)',':^)','=]','=)',\n",
    "                  ':‑D',':D','8‑D','8D','x‑D','xD','X‑D','XD','=D','=3','B^D',':-))']\n",
    "                \n",
    "    sad_emoji = [':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@','>:(']\n",
    "    \n",
    "    negative_words = ['doesn’t', 'isn’t', 'wasn’t', 'shouldn’t', 'wouldn’t', 'couldn’t', 'won’t', 'can’t', 'don’t']\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    singles = []\n",
    "    texts = []\n",
    "    tempText = ''\n",
    "    classes = []\n",
    "    aspectTerm = []\n",
    "    \n",
    "    for row in reader:\n",
    "        aspectTerm.append(tokenizer.tokenize(row[' aspect_term']))\n",
    "        \n",
    "        classes.append(row[' class'])\n",
    "        tempText = row[' text'].replace('[comma]',',').lower()\n",
    "               \n",
    "        for hem in happy_emoji:\n",
    "            if hem in tempText:\n",
    "                tempText = tempText.replace(hem, 'emojihappy')\n",
    "        \n",
    "        for sem in sad_emoji:\n",
    "            if sem in tempText:\n",
    "                tempText = tempText.replace(sem, 'emojisad')\n",
    "                \n",
    "        for negw in negative_words:\n",
    "            if negw in tempText:\n",
    "                tempText = tempText.replace(negw, 'not')\n",
    "        \n",
    "        singles.append(tokenizer.tokenize(tempText))\n",
    "        \n",
    "        texts.append(tempText)\n",
    "        \n",
    "    newTokenList = []\n",
    "    \n",
    "    '''\n",
    "    # stemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "        \n",
    "    for tokenList in singles:\n",
    "        newToken = []\n",
    "        for token in tokenList:\n",
    "            if token not in stop_words:\n",
    "                newToken.append(stemmer.stem(token))\n",
    "            if token == \"not\":\n",
    "                newToken.append(stemmer.stem(token))\n",
    "        newTokenList.append(newToken)\n",
    "    '''\n",
    "    \n",
    "    # lemmatize\n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for tokenList in singles:\n",
    "        newToken = []\n",
    "        for token in tokenList:\n",
    "            if token not in stop_words:\n",
    "                newToken.append(wnl.lemmatize(token))\n",
    "        newTokenList.append(newToken)\n",
    "    \n",
    "    # lemmatize aspect term\n",
    "    newApectTermList =[]\n",
    "    \n",
    "    for asp in aspectTerm:\n",
    "        newAT = []\n",
    "        for ap in asp:\n",
    "            newAT.append(wnl.lemmatize(ap))\n",
    "        newApectTermList.append(newAT)\n",
    "        \n",
    "    '''\n",
    "    unigrams = []\n",
    "    \n",
    "    for single in singles:\n",
    "        unigrams.append(ngrams(single, 1))\n",
    "    '''\n",
    "    # new review text\n",
    "    newText = []\n",
    "    \n",
    "    for tk in newTokenList:\n",
    "        newText.append(' '.join(tk))\n",
    "        \n",
    "    # new aspect term\n",
    "    newAspectTerm = []\n",
    "    \n",
    "    for apt in newApectTermList:\n",
    "        newAspectTerm.append(' '.join(apt))\n",
    "        \n",
    "    '''\n",
    "    kf = KFold(n_splits=10)\n",
    "    \n",
    "    \n",
    "    #vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    data_tfidf = count_vectorizer.fit_transform(newText)\n",
    "    tfidf_data = TfidfTransformer(use_idf=False).fit_transform(data_tfidf)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # POS tagging\n",
    "    st = StanfordPOSTagger('english-left3words-distsim.tagger')\n",
    "    \n",
    "    textPOS = []\n",
    "    \n",
    "    for nt in newText:\n",
    "        textPOS.append(st.tag(nt.split()))\n",
    "        \n",
    "    # save pos tags to txt file\n",
    "    f1 = open('postagsfull.txt','w')\n",
    "    for row in textPOS:\n",
    "        f1.write(str(row)+\"\\n\")\n",
    "    f1.close()\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # dependency parsing\n",
    "\n",
    "    depParsingList = []\n",
    "    \n",
    "    \n",
    "    for nt in newText[1900:2000]:\n",
    "        dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "        result = dep_parser.raw_parse(nt)    \n",
    "        dep = result.__next__()    \n",
    "        depParsingList.append(list(dep.triples()))\n",
    "        \n",
    "    \n",
    "    f2 = open('dependencyparsingfull7.txt','w')\n",
    "    for dp in depParsingList:\n",
    "        f2.write(str(dp)+\"\\n\")\n",
    "    f2.close()\n",
    "    \n",
    "    \n",
    "    # POS tagging\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "#print(textPOS[0])\n",
    "import sys\n",
    "sys.getsizeof(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train,data_test,target_train,target_test = cross_validation.train_test_split(tfidf_data,classes,test_size=0.4,random_state=43)\n",
    "#classifier = BernoulliNB().fit(data_train,target_train)\n",
    "#predicted = classifier.predict(data_test)\n",
    "#evaluate_model(target_test,predicted)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "predicted = cross_val_predict(clf, tfidf_data, classes, cv=10)\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(classes,predicted))\n",
    "print(\"The accuracy score is {:.2%}\".format(metrics.accuracy_score(classes, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "#from nltk.tag.stanford import CoreNLPPOSTagger\n",
    "#CoreNLPPOSTagger(url='http://localhost:9000').tag('What is the airspeed of an unladen swallow ?'.split())\n",
    "\n",
    "#import nltk\n",
    "#nltk.pos_tag(newTokenList[0])\n",
    "import csv\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "st = StanfordPOSTagger('english-left3words-distsim.tagger')\n",
    "newT = [\"obviously one important feature computer human interface\", \n",
    "         \"good every day computing web browsing\",\n",
    "         \"keyboard alright plate around cheap plastic make hollow sound using mouse command button\"]\n",
    "taggedT = []\n",
    "\n",
    "for n in newT:\n",
    "    taggedT.append(st.tag(n.split()))\n",
    "\n",
    "'''\n",
    "for tt in taggedT:\n",
    "    print(tt)\n",
    "'''\n",
    "\n",
    "'''\n",
    "out = open('out.csv', 'w')\n",
    "for row in taggedT:\n",
    "    out.write('%s;' % column)\n",
    "out.write('\\n')\n",
    "out.close()\n",
    "\n",
    "'''\n",
    "'''\n",
    "with open('out.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ']')\n",
    "    for row in taggedT:        \n",
    "        spamwriter.writerow(str(row))\n",
    "'''\n",
    "'''\n",
    "f = open('out.csv','w')\n",
    "for row in taggedT:        \n",
    "    f.write(str(row) + \"\\n\") \n",
    "f.close()\n",
    "'''\n",
    "f = open('postags.txt','w')\n",
    "for row in taggedT:\n",
    "    f.write(str(row)+\"\\n\")\n",
    "f.close()\n",
    "\n",
    "dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "\n",
    "depParsingList = []\n",
    "    \n",
    "for nt in newT:\n",
    "    result = dep_parser.raw_parse(nt)    \n",
    "    dep = result.__next__()    \n",
    "    depParsingList.append(list(dep.triples()))\n",
    "    \n",
    "for dp in depParsingList:\n",
    "    print(dp)\n",
    "    \n",
    "f2 = open('dependencyparsing.txt','w')\n",
    "for dp in depParsingList:\n",
    "    f2.write(str(dp)+\"\\n\")\n",
    "f.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "print([parse.tree() for parse in dep_parser.raw_parse(\"The quick brown fox jumps over the lazy dog.\")])\n",
    "result = dep_parser.raw_parse(\"The pizza at the restaurant was very good.\")\n",
    "dep = result.__next__()\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "import os\n",
    "os.environ['STANFORD_MODELS'] + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#experiment\n",
    "#from collections import Counter\n",
    "'''\n",
    "for text in texts:\n",
    "    print('\\n'+text)\n",
    "'''\n",
    "'''\n",
    "for sss in singles:\n",
    "    print(sss)\n",
    "'''\n",
    "'''\n",
    "for tk in newTokenList:\n",
    "    print(' '.join(tk))\n",
    "'''\n",
    "#print(newTokenList[0])\n",
    "'''\n",
    "for tk in newTokenList:\n",
    "    print(\"\\n\",tk)\n",
    "'''\n",
    "\n",
    "'''\n",
    "#print(Counter(unigrams))\n",
    "\n",
    "for grams in unigrams:\n",
    "    #print(Counter(grams))\n",
    "    for g in grams:\n",
    "         print(g)\n",
    "'''\n",
    "\n",
    "#len(classes)\n",
    "\n",
    "for nt in newText:\n",
    "    print(\"\\n\",nt)\n",
    "\n",
    "\n",
    "'''\n",
    "for tokenList in singles:\n",
    "        for token in tokenList:\n",
    "            print(token)\n",
    "        print(\"\\n\")\n",
    "'''\n",
    "\n",
    "'''\n",
    "for at in newAspectTerm:\n",
    "    print(\"\\n\",at)\n",
    "'''\n",
    "'''\n",
    "for dp in depParsingList:\n",
    "    print(dp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big experiment\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# review.csv contains two columns\n",
    "# first column is the review content (quoted)\n",
    "# second column is the assigned sentiment (positive or negative)\n",
    "def load_file():\n",
    "    with open('review.csv') as csv_file:\n",
    "        reader = csv.reader(csv_file,delimiter=\",\",quotechar='\"')\n",
    "        next(reader)\n",
    "        data =[]\n",
    "        target = []\n",
    "        for row in reader:\n",
    "            # skip missing data\n",
    "            if row[0] and row[1]:\n",
    "                data.append(row[0])\n",
    "                target.append(row[1])\n",
    "\n",
    "        return data,target\n",
    "\n",
    "# preprocess creates the term frequency matrix for the review data set\n",
    "def preprocess():\n",
    "    data,target = load_file()\n",
    "    count_vectorizer = CountVectorizer(binary='true')\n",
    "    data = count_vectorizer.fit_transform(data)\n",
    "    tfidf_data = TfidfTransformer(use_idf=False).fit_transform(data)\n",
    "\n",
    "    return tfidf_data\n",
    "\n",
    "def learn_model(data,target):\n",
    "    # preparing data for split validation. 60% training, 40% test\n",
    "    data_train,data_test,target_train,target_test = cross_validation.train_test_split(data,target,test_size=0.4,random_state=43)\n",
    "    classifier = BernoulliNB().fit(data_train,target_train)\n",
    "    predicted = classifier.predict(data_test)\n",
    "    evaluate_model(target_test,predicted)\n",
    "\n",
    "# read more about model evaluation metrics here\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "def evaluate_model(target_true,target_predicted):\n",
    "    print(classification_report(target_true,target_predicted))\n",
    "    print(\"The accuracy score is {:.2%}\".format(accuracy_score(target_true,target_predicted)))\n",
    "\n",
    "def main():\n",
    "    data,target = load_file()\n",
    "    tf_idf = preprocess()\n",
    "    learn_model(tf_idf,target)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "\n",
    "with open('dara 1_train.csv', newline='') as csvfile2:\n",
    "    reader2 = csv.DictReader(csvfile2)\n",
    "    \n",
    "    data2,target2 = load_file()\n",
    "                count_vectorizer = CountVectorizer(binary='true')\n",
    "                data = count_vectorizer.fit_transform(data)\n",
    "                tfidf_data = TfidfTransformer(use_idf=False).\n",
    "                fit_transform(data)\n",
    "\n",
    "                return tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "exps = ['asd','asd_','_wqeqwe']\n",
    "expsf = []\n",
    "for ex in exps:\n",
    "    expsf.append(ex.replace('_',''))\n",
    "print(expsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "lhap = [':)',':P',':D']\n",
    "lsad = [':(',':|']\n",
    "t1 = \"i am happy :), but he is sad :|\"\n",
    "t2 = \"i am sad :(, he is :P\"\n",
    "t1.replace(lhap,'smilehappy')\n",
    "t1.replace(lsad,'smilesad')\n",
    "t2.replace(lhap,'smilehappy')\n",
    "t2.replace(lsad,'smilesad')\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "\n",
    "happy_emoji = [':‑)',':)',':-]',':]',':-3',':3',':->',':>','8-)','8)',':-}', ':}',':o)',':c)',':^)','=]','=)',\n",
    "                  ':‑D',':D','8‑D','8D','x‑D','xD','X‑D','XD','=D','=3','B^D',':-))']\n",
    "                \n",
    "sad_emoji = [':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@','>:(']\n",
    "\n",
    "strr = \"i am happy :)\"\n",
    "for hem in happy_emoji:\n",
    "    if hem in strr:\n",
    "        strr = strr.replace(hem, 'emojihappy')\n",
    "\n",
    "for sem in sad_emoji:\n",
    "    strr.replace(sem, 'emojisad')\n",
    "    \n",
    "print(strr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
