{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanku\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import svm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "with open('dara 1_train.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    \"\"\"\n",
    "    for row in reader:\n",
    "        print(row['example_id'])\n",
    "        print(row[' text'])\n",
    "        print(row[' aspect_term'])\n",
    "        print(row[' term_location'])\n",
    "        print(row[' class'])\n",
    "        print()        \n",
    "    \"\"\"\n",
    "    happy_emoji = [':‑)',':)',':-]',':]',':-3',':3',':->',':>','8-)','8)',':-}', ':}',':o)',':c)',':^)','=]','=)',\n",
    "                  ':‑D',':D','8‑D','8D','x‑D','xD','X‑D','XD','=D','=3','B^D',':-))']\n",
    "                \n",
    "    sad_emoji = [':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@','>:(']\n",
    "    \n",
    "    negative_words = ['doesn’t', 'isn’t', 'wasn’t', 'shouldn’t', 'wouldn’t', 'couldn’t', 'won’t', 'can’t', 'don’t']\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    singles = []\n",
    "    texts = []\n",
    "    tempText = ''\n",
    "    classes = []\n",
    "    aspectTerm = []\n",
    "    \n",
    "    for row in reader:\n",
    "        aspectTerm.append(tokenizer.tokenize(row[' aspect_term']))\n",
    "        \n",
    "        classes.append(row[' class'])\n",
    "        tempText = row[' text'].replace('[comma]',',').lower()\n",
    "               \n",
    "        for hem in happy_emoji:\n",
    "            if hem in tempText:\n",
    "                tempText = tempText.replace(hem, 'emojihappy')\n",
    "        \n",
    "        for sem in sad_emoji:\n",
    "            if sem in tempText:\n",
    "                tempText = tempText.replace(sem, 'emojisad')\n",
    "                \n",
    "        for negw in negative_words:\n",
    "            if negw in tempText:\n",
    "                tempText = tempText.replace(negw, 'not')\n",
    "        \n",
    "        singles.append(tokenizer.tokenize(tempText))\n",
    "        \n",
    "        texts.append(tempText)\n",
    "        \n",
    "    newTokenList = []\n",
    "    \n",
    "    '''\n",
    "    # stemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "        \n",
    "    for tokenList in singles:\n",
    "        newToken = []\n",
    "        for token in tokenList:\n",
    "            if token not in stop_words:\n",
    "                newToken.append(stemmer.stem(token))\n",
    "            if token == \"not\":\n",
    "                newToken.append(stemmer.stem(token))\n",
    "        newTokenList.append(newToken)\n",
    "    '''\n",
    "    \n",
    "    # lemmatize\n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for tokenList in singles:\n",
    "        newToken = []\n",
    "        for token in tokenList:\n",
    "            if token not in stop_words:\n",
    "                newToken.append(wnl.lemmatize(token))\n",
    "        newTokenList.append(newToken)\n",
    "    \n",
    "    # lemmatize aspect term\n",
    "    newApectTermList =[]\n",
    "    \n",
    "    for asp in aspectTerm:\n",
    "        newAT = []\n",
    "        for ap in asp:\n",
    "            newAT.append(wnl.lemmatize(ap))\n",
    "        newApectTermList.append(newAT)\n",
    "        \n",
    "    '''\n",
    "    unigrams = []\n",
    "    \n",
    "    for single in singles:\n",
    "        unigrams.append(ngrams(single, 1))\n",
    "    '''\n",
    "    # new review text\n",
    "    newText = []\n",
    "    \n",
    "    for tk in newTokenList:\n",
    "        newText.append(' '.join(tk))\n",
    "        \n",
    "    # new aspect term\n",
    "    newAspectTerm = []\n",
    "    \n",
    "    for apt in newApectTermList:\n",
    "        newAspectTerm.append(' '.join(apt))\n",
    "        \n",
    "    '''\n",
    "    kf = KFold(n_splits=10)\n",
    "    \n",
    "    \n",
    "    #vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    data_tfidf = count_vectorizer.fit_transform(newText)\n",
    "    tfidf_data = TfidfTransformer(use_idf=False).fit_transform(data_tfidf)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # POS tagging\n",
    "    st = StanfordPOSTagger('english-left3words-distsim.tagger')\n",
    "    \n",
    "    textPOS = []\n",
    "    \n",
    "    for nt in newText:\n",
    "        textPOS.append(st.tag(nt.split()))\n",
    "        \n",
    "    # save pos tags to txt file\n",
    "    f1 = open('postagsfull.txt','w')\n",
    "    for row in textPOS:\n",
    "        f1.write(str(row)+\"\\n\")\n",
    "    f1.close()\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # dependency parsing\n",
    "\n",
    "    depParsingList = []\n",
    "    \n",
    "    \n",
    "    for nt in newText[1900:2000]:\n",
    "        dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "        result = dep_parser.raw_parse(nt)    \n",
    "        dep = result.__next__()    \n",
    "        depParsingList.append(list(dep.triples()))\n",
    "        \n",
    "    \n",
    "    f2 = open('dependencyparsingfull7.txt','w')\n",
    "    for dp in depParsingList:\n",
    "        f2.write(str(dp)+\"\\n\")\n",
    "    f2.close()\n",
    "    \n",
    "    \n",
    "    # POS tagging\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "#print(textPOS[0])\n",
    "import sys\n",
    "sys.getsizeof(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train,data_test,target_train,target_test = cross_validation.train_test_split(tfidf_data,classes,test_size=0.4,random_state=43)\n",
    "#classifier = BernoulliNB().fit(data_train,target_train)\n",
    "#predicted = classifier.predict(data_test)\n",
    "#evaluate_model(target_test,predicted)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "predicted = cross_val_predict(clf, tfidf_data, classes, cv=10)\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(classes,predicted))\n",
    "print(\"The accuracy score is {:.2%}\".format(metrics.accuracy_score(classes, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "#from nltk.tag.stanford import CoreNLPPOSTagger\n",
    "#CoreNLPPOSTagger(url='http://localhost:9000').tag('What is the airspeed of an unladen swallow ?'.split())\n",
    "\n",
    "#import nltk\n",
    "#nltk.pos_tag(newTokenList[0])\n",
    "import csv\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "st = StanfordPOSTagger('english-left3words-distsim.tagger')\n",
    "newT = [\"obviously one important feature computer human interface\", \n",
    "         \"good every day computing web browsing\",\n",
    "         \"keyboard alright plate around cheap plastic make hollow sound using mouse command button\"]\n",
    "taggedT = []\n",
    "\n",
    "for n in newT:\n",
    "    taggedT.append(st.tag(n.split()))\n",
    "\n",
    "'''\n",
    "for tt in taggedT:\n",
    "    print(tt)\n",
    "'''\n",
    "\n",
    "'''\n",
    "out = open('out.csv', 'w')\n",
    "for row in taggedT:\n",
    "    out.write('%s;' % column)\n",
    "out.write('\\n')\n",
    "out.close()\n",
    "\n",
    "'''\n",
    "'''\n",
    "with open('out.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ']')\n",
    "    for row in taggedT:        \n",
    "        spamwriter.writerow(str(row))\n",
    "'''\n",
    "'''\n",
    "f = open('out.csv','w')\n",
    "for row in taggedT:        \n",
    "    f.write(str(row) + \"\\n\") \n",
    "f.close()\n",
    "'''\n",
    "f = open('postags.txt','w')\n",
    "for row in taggedT:\n",
    "    f.write(str(row)+\"\\n\")\n",
    "f.close()\n",
    "\n",
    "dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "\n",
    "depParsingList = []\n",
    "    \n",
    "for nt in newT:\n",
    "    result = dep_parser.raw_parse(nt)    \n",
    "    dep = result.__next__()    \n",
    "    depParsingList.append(list(dep.triples()))\n",
    "    \n",
    "for dp in depParsingList:\n",
    "    print(dp)\n",
    "    \n",
    "f2 = open('dependencyparsing.txt','w')\n",
    "for dp in depParsingList:\n",
    "    f2.write(str(dp)+\"\\n\")\n",
    "f.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "print([parse.tree() for parse in dep_parser.raw_parse(\"The quick brown fox jumps over the lazy dog.\")])\n",
    "result = dep_parser.raw_parse(\"The pizza at the restaurant was very good.\")\n",
    "dep = result.__next__()\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "import os\n",
    "os.environ['STANFORD_MODELS'] + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#experiment\n",
    "#from collections import Counter\n",
    "'''\n",
    "for text in texts:\n",
    "    print('\\n'+text)\n",
    "'''\n",
    "'''\n",
    "for sss in singles:\n",
    "    print(sss)\n",
    "'''\n",
    "'''\n",
    "for tk in newTokenList:\n",
    "    print(' '.join(tk))\n",
    "'''\n",
    "#print(newTokenList[0])\n",
    "'''\n",
    "for tk in newTokenList:\n",
    "    print(\"\\n\",tk)\n",
    "'''\n",
    "\n",
    "'''\n",
    "#print(Counter(unigrams))\n",
    "\n",
    "for grams in unigrams:\n",
    "    #print(Counter(grams))\n",
    "    for g in grams:\n",
    "         print(g)\n",
    "'''\n",
    "\n",
    "#len(classes)\n",
    "\n",
    "for nt in newText:\n",
    "    print(\"\\n\",nt)\n",
    "\n",
    "\n",
    "'''\n",
    "for tokenList in singles:\n",
    "        for token in tokenList:\n",
    "            print(token)\n",
    "        print(\"\\n\")\n",
    "'''\n",
    "\n",
    "'''\n",
    "for at in newAspectTerm:\n",
    "    print(\"\\n\",at)\n",
    "'''\n",
    "'''\n",
    "for dp in depParsingList:\n",
    "    print(dp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big experiment\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# review.csv contains two columns\n",
    "# first column is the review content (quoted)\n",
    "# second column is the assigned sentiment (positive or negative)\n",
    "def load_file():\n",
    "    with open('review.csv') as csv_file:\n",
    "        reader = csv.reader(csv_file,delimiter=\",\",quotechar='\"')\n",
    "        next(reader)\n",
    "        data =[]\n",
    "        target = []\n",
    "        for row in reader:\n",
    "            # skip missing data\n",
    "            if row[0] and row[1]:\n",
    "                data.append(row[0])\n",
    "                target.append(row[1])\n",
    "\n",
    "        return data,target\n",
    "\n",
    "# preprocess creates the term frequency matrix for the review data set\n",
    "def preprocess():\n",
    "    data,target = load_file()\n",
    "    count_vectorizer = CountVectorizer(binary='true')\n",
    "    data = count_vectorizer.fit_transform(data)\n",
    "    tfidf_data = TfidfTransformer(use_idf=False).fit_transform(data)\n",
    "\n",
    "    return tfidf_data\n",
    "\n",
    "def learn_model(data,target):\n",
    "    # preparing data for split validation. 60% training, 40% test\n",
    "    data_train,data_test,target_train,target_test = cross_validation.train_test_split(data,target,test_size=0.4,random_state=43)\n",
    "    classifier = BernoulliNB().fit(data_train,target_train)\n",
    "    predicted = classifier.predict(data_test)\n",
    "    evaluate_model(target_test,predicted)\n",
    "\n",
    "# read more about model evaluation metrics here\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "def evaluate_model(target_true,target_predicted):\n",
    "    print(classification_report(target_true,target_predicted))\n",
    "    print(\"The accuracy score is {:.2%}\".format(accuracy_score(target_true,target_predicted)))\n",
    "\n",
    "def main():\n",
    "    data,target = load_file()\n",
    "    tf_idf = preprocess()\n",
    "    learn_model(tf_idf,target)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "\n",
    "with open('dara 1_train.csv', newline='') as csvfile2:\n",
    "    reader2 = csv.DictReader(csvfile2)\n",
    "    \n",
    "    data2,target2 = load_file()\n",
    "                count_vectorizer = CountVectorizer(binary='true')\n",
    "                data = count_vectorizer.fit_transform(data)\n",
    "                tfidf_data = TfidfTransformer(use_idf=False).\n",
    "                fit_transform(data)\n",
    "\n",
    "                return tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "exps = ['asd','asd_','_wqeqwe']\n",
    "expsf = []\n",
    "for ex in exps:\n",
    "    expsf.append(ex.replace('_',''))\n",
    "print(expsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "lhap = [':)',':P',':D']\n",
    "lsad = [':(',':|']\n",
    "t1 = \"i am happy :), but he is sad :|\"\n",
    "t2 = \"i am sad :(, he is :P\"\n",
    "t1.replace(lhap,'smilehappy')\n",
    "t1.replace(lsad,'smilesad')\n",
    "t2.replace(lhap,'smilehappy')\n",
    "t2.replace(lsad,'smilesad')\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment\n",
    "\n",
    "happy_emoji = [':‑)',':)',':-]',':]',':-3',':3',':->',':>','8-)','8)',':-}', ':}',':o)',':c)',':^)','=]','=)',\n",
    "                  ':‑D',':D','8‑D','8D','x‑D','xD','X‑D','XD','=D','=3','B^D',':-))']\n",
    "                \n",
    "sad_emoji = [':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@','>:(']\n",
    "\n",
    "strr = \"i am happy :)\"\n",
    "for hem in happy_emoji:\n",
    "    if hem in strr:\n",
    "        strr = strr.replace(hem, 'emojihappy')\n",
    "\n",
    "for sem in sad_emoji:\n",
    "    strr.replace(sem, 'emojisad')\n",
    "    \n",
    "print(strr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
